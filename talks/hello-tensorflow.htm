<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
<link rel="stylesheet" href="talks.css">
<div class="body">
  <title>Hello Tensorflow.js</title>
  <div class ="title">Hello TensorFlow.js</div><br><br><br><br>
  <b>Machine Learning (ML)</b> is the dope new thing that everyone‚Äôs talking about, because it‚Äôs really good at learning from data so that it can predict similar things in the future. Doing ML by hand is pretty annoying since it usually involves matrix math which is zero fun in JavaScript (or if you ask me: anywhere üòÖ).
  Thankfully, <a class='click' href='https://js.tensorflow.org' target='blank'>TensorFlow.js</a> is here to help! It‚Äôs an open source library that has a lot of built-in Machine Learning-y things like models and algorithms so that you don‚Äôt have to write them from scratch.
  <br><br>
  <span class='heading'>How it works</span><br><br>
  Most machine learning algorithms follow this pattern:</br>
  <ul>
    <li>We have to figure out the <b>‚Äúfeatures‚Äù</b> of the secret formula that generated the data we were given, so that we can learn them. In my opinion, this is like 80% of the complexity of solving an ML problem. In this example, we were told the shape of the secret formula (it‚Äôs a cubic!), so the features we have to learn are the coefficients in the polynomial. For something more complex like the ‚Äúis this a dog or a blueberry muffin‚Äù problem, we‚Äôd have to look at pixels and colours and formations and what makes a dog a dog and not a muffin.
      <li>Once we figure out these features (in our case, those a,b,c,d coefficients), we initialize them to some random values. We could now use them to make predictions, but they would be teeeeeerrible because they‚Äôre just random.<li>(I‚Äôm just going to use our actual example from now on and not dogs)<li>We start looking at every piece (x,y) of training data we were given. We take the x value, and based on these coefficients we have estimated, we predict what the y value would be. We then look at the correct y value from the original training data, calculate the difference between the two, and then adjust our coefficients so that our predicted value gets closer to the correct one.
        <li>(this, with more math sprinkled in is called ‚Äústochastic gradient descent‚Äù. ‚ÄúStochastic‚Äù means probabilistic, and ‚Äúgradient descent‚Äù should make you think of walking down a hill, towards a sink hole ‚Äì the higher the hill, the bigger the prediction error, which is why you want to descend towards the error-free hole.)<li>This part of code is actually pretty messy (because matrices and derivatives), and TensorFlow does this for us!<li>We keep doing this until we use up all the data, and then repeat the entire process so that we iterate over the same data over and over again until at the end we‚Äôve pretty much learnt the coefficients!</ul>
          <br><br>
          <span class='heading'>The Code</span><br>
          <br>You can look at the code for the demo <a class='click' href='https://www.github.com/notshekhar/convolutionalNeuralNetwork' target='blank'>on GitHub</a>.
          I tried to comment most lines of the code with either what the algorithm or TensorFlow are doing (especially when TensorFlow is actually doing a looooot of heavy lifting behind the scenes). I hope it helps!
          <br><br><br>
          <div class="thankyou">THANK YOU FOR READING ‚ù§Ô∏é</div>
</div>
